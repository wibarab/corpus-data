{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34c743a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ELAN to TEI conversion \n",
    "\n",
    "**Author:** Daniel Schopper    \n",
    "**Description:** This notebook automates the ELAN to TEI conversion in the WIBARAB Project. It is based on the same process in the SHAWI Project.\n",
    "**Last Change:** 2023-10-10     \n",
    "**History:**    \n",
    "* 2023-10-10: Initital set up\n",
    "* 2023-10-12: updated to Saxon CE HE (Omar Siam)\n",
    "* 2025-11-01: update way of fetching metadata spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from azure.identity import ClientSecretCredential\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import pathlib\n",
    "#import filetype â€“ not used\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlsplit\n",
    "from lxml import isoschematron, etree\n",
    "import saxonche\n",
    "from zipfile import ZipFile\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "# from inspect import getmembers, signature\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd189fd",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d7a8e-fa0d-4c83-afdb-7fcf3430c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the URL of the Sharepoint installation \n",
    "sp_baseURL = \"oeawacat.sharepoint.com\"\n",
    "\n",
    "# the sharepoint username + password are taken from the environment\n",
    "sp_username = os.environ['SP_USERNAME']\n",
    "sp_pwd = pwd = os.environ['SP_PWD']\n",
    "\n",
    "# the name of the Sharepoint Site\n",
    "sp_siteName = \"ACDH-CH_p_WIBARAB_BedoinTypeArabicNomadicSedentaryPeopleMidd\"\n",
    "\n",
    "# the path to the Excel file\n",
    "# sp_pathToRecordingsXLSX = \"Shared%20Documents/Fieldwork%20data%20+%20analysis/WIBARAB_Recordings.xlsx\"\n",
    "sp_RecordingsXSLX = \"WIBARAB_Recordings.xlsx\" # is searched for recursively on SharePoint\n",
    "\n",
    "\n",
    "# the name of the local directory where downloaded data will be stored\n",
    "dataDir = \"data\"\n",
    "\n",
    "# the name of the local directory where downloaded libraries and other auxiliary code will be stored\n",
    "libDir = \"lib\"\n",
    "\n",
    "# the root of the git repository\n",
    "dataHomeDir = \"../..\"\n",
    "\n",
    "# path to project-specific stylesheets\n",
    "pathToStylesheetsDir = dataHomeDir+\"/082_scripts_xsl\"\n",
    "\n",
    "# the path to the ELAN transcription files\n",
    "pathToELANDir = dataHomeDir+\"/122_elan\"\n",
    "\n",
    "# the path to the non-annotated TEI transcription files\n",
    "pathToTEIDir = dataHomeDir+\"/103_tei_w\"\n",
    "\n",
    "# the path to the annotated TEI transcription files\n",
    "pathToAnnotatedTEIDir = dataHomeDir+\"/010_manannot\"\n",
    "\n",
    "\n",
    "# the path to the NoSkE verticals\n",
    "noSkEVertDir = dataHomeDir+\"/130_vert_plain\"\n",
    "\n",
    "# the path to the tei Corpus document produced by this script\n",
    "pathToTeiCorpus = pathToTEIDir+\"/wibarabCorpus.xml\"\n",
    "\n",
    "\n",
    "# the path to the audio files\n",
    "pathToRecordingsDir = \"THIS_IS_NOT_USED\"#\"/mnt/univie_orientalistik/SHAWI/Recordings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c13ae0-980e-4513-bac2-eb8f5bb8b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteOutputOfPreviousRuns(pathToDir):\n",
    "    # TOOD Implement\n",
    "    logging.info(\"removing output of previous runs in \"+pathToDir)\n",
    "    if not pathToDir.startswith(\"..\"):\n",
    "        logging.error(pathToDir +\" is not a relative path. Aborting deletion.\")\n",
    "    else:\n",
    "        if os.path.exists(pathToDir):\n",
    "            with os.scandir(pathToDir) as it:\n",
    "                for entry in it:\n",
    "                    if entry.is_file() and entry.name.endswith('.xml') or entry.name.endswith('.txt'):\n",
    "                        os.remove(entry)\n",
    "        else:#\n",
    "            logging.info(\"could not find directory \"+pathToDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88993602-1207-4906-9325-071e594b99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with saxonche.PySaxonProcessor(license=False) as proc:\n",
    "    logging.info(proc.version)\n",
    "    proc.set_cwd(os.path.dirname(os.path.abspath('')))\n",
    "    logging.info(proc.cwd)\n",
    "\n",
    "\n",
    "#set up directories\n",
    "logging.info(\"** setting up directories **\")\n",
    "\n",
    "# remove data from previous runs\n",
    "deleteOutputOfPreviousRuns(pathToTEIDir)\n",
    "deleteOutputOfPreviousRuns(noSkEVertDir)\n",
    "\n",
    "for i in [dataDir,libDir]: \n",
    "    if os.path.exists(i):\n",
    "        logging.info(\"skipped existing directory '\"+i+\"'\")\n",
    "    else:\n",
    "        os.mkdir(i)\n",
    "        logging.info(\"created directory '\"+i+\"'\")\n",
    "        \n",
    "        \n",
    "# define which steps should be skipped. \n",
    "\n",
    "SKIP_PROCESSING = []#[\"runTEICorpo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfda0dc",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Step 1: get the latest release of the TEI Stylesheets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960d582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# fetch the TEI Stylesheets    \n",
    "def installFromGithub(libraryName):\n",
    "    auth = {}\n",
    "    if 'GITHUB_TOKEN' in os.environ:\n",
    "        auth = {\"Authorization\": \"Bearer \"+os.environ['GITHUB_TOKEN']}\n",
    "    headers = {\"Accept\" : \"application/vnd.github.v3+json\"}\n",
    "    repo = libraryName\n",
    "    logging.info(\"** Fetching library \"+repo+\" **\")\n",
    "    libBasePath = libDir+\"/\"+repo\n",
    "    \n",
    "    # First we check which tag name the latest release has\n",
    "    r = requests.get(\"https://api.github.com/repos/\"+repo+\"/releases/latest\", headers={**headers, **auth})\n",
    "    if r.status_code != 200:\n",
    "        logging.error(\"An error occured fetching the latest release. Maybe there isn't any release? \")\n",
    "        logging.error(r.content)\n",
    "        return 1\n",
    "    release = r.json()\n",
    "    tag = release[\"tag_name\"]\n",
    "    \n",
    "    # we check whether we have the latest version already \\\n",
    "    # by checking if the respective path is already installed\n",
    "    libReleasePath = libBasePath+\"/\"+tag\n",
    "    haveLatestVersion = os.path.exists(libReleasePath)\n",
    "    if haveLatestVersion:\n",
    "        logging.info(\"We have already the latest version (\"+tag+\"). Exiting\")\n",
    "        logging.info(\"\")\n",
    "        return libReleasePath\n",
    "    else:\n",
    "        url = release[\"assets\"][0][\"browser_download_url\"]\n",
    "        payload = requests.get(url, headers=auth).content\n",
    "        zipfilename = os.path.basename(url)\n",
    "        os.makedirs(libReleasePath, exist_ok=True)\n",
    "        zipfilePath = libReleasePath +\"/\"+zipfilename\n",
    "        open(zipfilePath, 'wb').write(payload)\n",
    "        ZipFile(zipfilePath).extractall(path=libReleasePath)\n",
    "        logging.info(\"Downloaded latest version (\"+tag+\") to \"+libReleasePath)\n",
    "        logging.info(\"\")\n",
    "        return libReleasePath\n",
    "\n",
    "\n",
    "pathToTEIGuidelines=installFromGithub(\"TEIC/TEI\")\n",
    "pathToTEIStylesheets=installFromGithub(\"TEIC/Stylesheets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c41966",
   "metadata": {},
   "source": [
    "### Step 2: Download the latest version of the Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ce3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharePointDownloader:\n",
    "    def __init__(self, credentials, site_name, team_name):\n",
    "        self.client_credential = credentials\n",
    "        site_id = self._get_site_id_by_name(site_name, team_name)\n",
    "        self.site_id = site_id\n",
    "\n",
    "    def _get_token(self):\n",
    "        return self.client_credential.get_token(\n",
    "            \"https://graph.microsoft.com/.default\"\n",
    "        ).token\n",
    "\n",
    "    def _get_graph_headers(self):\n",
    "        token = self._get_token()\n",
    "        return {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def _get_site_id_by_name(self, site_name, team_name):\n",
    "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_name}:/sites/{team_name}\"\n",
    "        response = requests.get(url, headers=self._get_graph_headers())\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"id\")\n",
    "\n",
    "    def _get_drive_id(self):\n",
    "        url = f\"https://graph.microsoft.com/v1.0/sites/{self.site_id}/drive\"\n",
    "        response = requests.get(url, headers=self._get_graph_headers())\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"id\")\n",
    "\n",
    "    def _list_content(self, headers, drive_id, item_id=\"root\", source_file_name=None):\n",
    "        items_url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/items/{item_id}/children\"\n",
    "        response = requests.get(items_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        items = response.json().get(\"value\", [])\n",
    "        \n",
    "        for item in items:\n",
    "            if \"folder\" in item:\n",
    "                result = self._list_content(headers, drive_id, item[\"id\"], source_file_name)\n",
    "                if result:\n",
    "                    return result\n",
    "            if item[\"name\"] == source_file_name:\n",
    "                return item\n",
    "        return None\n",
    "    \n",
    "    def download_file(self, file_name):\n",
    "        drive_id = self._get_drive_id()\n",
    "        headers = self._get_graph_headers()\n",
    "        file_item = self._list_content(headers, drive_id, source_file_name=file_name)\n",
    "        \n",
    "        if not file_item:\n",
    "            raise FileNotFoundError(f\"{file_name} not found.\")\n",
    "\n",
    "        file_url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/items/{file_item['id']}/content\"\n",
    "        response = requests.get(file_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.content\n",
    "\n",
    "\n",
    "pathToExcelSheet = \"data/WIBARAB_Recordings.xlsx\"\n",
    "logging.info(pathToExcelSheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9687813",
   "metadata": {},
   "source": [
    "## Step 2: transform xlsx to TEI table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(s, xsl, o, parameters=[]):\n",
    "    # processor keeps files open on Windows and in doing so prevents moving or copying them\n",
    "    with saxonche.PySaxonProcessor(license=False) as proc:\n",
    "        proc.set_configuration_property(\"xi\", \"on\")\n",
    "        saxon = proc.new_xslt30_processor()\n",
    "        for i in parameters:\n",
    "            saxon.set_parameter(name=i, value=proc.make_string_value(parameters[i]))\n",
    "        try:\n",
    "            exec = saxon.compile_stylesheet(stylesheet_file=os.path.abspath(xsl))\n",
    "            exec.set_global_context_item(file_name=os.path.abspath(s))\n",
    "            # From the docs saxonc.html#PyXsltExecutable-set_initial_match_selection\n",
    "            # This method does not set the global context item for the transformation;\n",
    "            # if that is required, it can be done separately using the set_global_context_item method.\n",
    "            exec.apply_templates_returning_file(source_file=os.path.abspath(s), output_file=os.path.abspath(o))\n",
    "        except saxonche.PySaxonApiError as e:\n",
    "            logging.info(str(e))\n",
    "            logging.info(os.path.abspath(s)+\" - \"+os.path.abspath(xsl)+\" -> \"+os.path.abspath(o)+\" failed\")\n",
    "        if proc.exception_occurred:\n",
    "            logging.info(proc.get_error_message())\n",
    "            logging.info(os.path.abspath(s)+\" - \"+os.path.abspath(xsl)+\" -> \"+os.path.abspath(o)+\" failed\")\n",
    "        if os.path.exists(os.path.abspath(o)):\n",
    "            return o\n",
    "        else: \n",
    "            logging.info(\"there was an error transforming \"+s+\" with stylesheet \"+xsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531fda65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xlsx2teitable(xlsx, output):\n",
    "\n",
    "    # first, extract contents of XLSX document to a temp directory\n",
    "    unzipPath=xlsx.replace(\".xlsx\",\"\")\n",
    "    os.makedirs(unzipPath, exist_ok=True)\n",
    "    ZipFile(xlsx).extractall(path=unzipPath)\n",
    "    \n",
    "    # then transform the .rels file using the TEIC Stylesheets \n",
    "    pathToXlsxtoteiXSL=pathToTEIStylesheets+\"/xml/tei/stylesheet/xlsx/xlsxtotei.xsl\"\n",
    "\n",
    "    params = {\n",
    "        \"inputDir\" : pathlib.Path(os.path.abspath(unzipPath)).as_uri(),\n",
    "        \"workDir\" : pathlib.Path(os.path.abspath(unzipPath)).as_uri()\n",
    "    }\n",
    "\n",
    "    transform(\n",
    "        s = unzipPath+\"/_rels/.rels\", \n",
    "        xsl = pathToXlsxtoteiXSL, \n",
    "        o = output, \n",
    "        parameters=params\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041e152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pathToTEItable=pathToExcelSheet.replace(\".xlsx\",\".xml\")\n",
    "\n",
    "if not \"xlsx2teitable\" in SKIP_PROCESSING:    \n",
    "    xlsx2teitable(xlsx=pathToExcelSheet, output=pathToTEItable)\n",
    "    debugstring=\"\"\"<!-- \n",
    "   THIS FILE IS INCLUDED IN THE GIT REPOSITORY ONLY FOR DEBUGGING PURPOSES. \n",
    "   \n",
    "   The source of this file is constantly being edited at \n",
    "   https://oeawacat.sharepoint.com/:x:/r/sites/ACDH-CH_p_WIBARAB_BedoinTypeArabicNomadicSedentaryPeopleMidd/Shared%20Documents/Fieldwork%20data%20+%20analysis/WIBARAB_Recordings.xlsx?d=wa827dc0ab3ac472c8dcf059661394d96&csf=1&web=1&e=0JhzC6\n",
    "   So this copy is most probably already outdated.\n",
    "   \n",
    "  To update it, you can either run https://github.com/wibarab/corpus-data/blob/main/080_scripts_generic/080_01_ELAN2TEI/ELAN2TEI.ipynb\n",
    "   *OR*  \n",
    "   1) download the Excel file manually from Sharepoint\n",
    "   2) and tranform it to TEI using oxgarage.tei-c.org/ \n",
    "   \n",
    "-->\n",
    "    \"\"\"\n",
    "    f = open(pathToTEItable,mode=\"r\",encoding=\"UTF8\")\n",
    "    src = f.read()\n",
    "    new = src.replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>','<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'+debugstring)\n",
    "    f.close()\n",
    "    f = open(pathToTEItable, mode=\"wt\",encoding=\"UTF8\")\n",
    "    f.write(new)\n",
    "    f.close()\n",
    "        \n",
    "    logging.info(pathToTEItable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca2fcd",
   "metadata": {},
   "source": [
    "## Step 3: transform TEI table to corpus header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0889b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToTeitableToCorpusXSL=pathToStylesheetsDir+\"/table2corpus.xsl\"\n",
    "params = {\n",
    "    \"pathToRecordings\" : pathlib.Path(os.path.abspath(pathToRecordingsDir)).as_uri(),\n",
    "    # \"sp_pathToRecordingsXLSX\": sp_pathToRecordingsXLSX\n",
    "}\n",
    "try:\n",
    "    transform(pathToTEItable, pathToTeitableToCorpusXSL, pathToTeiCorpus, params)\n",
    "except saxonche.PySaxonApiError as e:\n",
    "    logging.error(\"an error occured: \" + str(e) + \"\\n\" + pathToTEItable + \": \" + pathToTeitableToCorpusXSL + \" -> \" + pathToTeiCorpus)\n",
    "logging.info(pathToTeiCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd00c7",
   "metadata": {},
   "source": [
    "## Step 4: Run TEICorpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def installFromUrl(url, force=False):\n",
    "    r = requests.get(url)\n",
    "    filename = os.path.basename(urlsplit(url).path)\n",
    "    downloadpath = libDir+\"/\"+filename\n",
    "    if os.path.exists(downloadpath) and not force:\n",
    "        logging.info(\"skipping download\")\n",
    "    else:\n",
    "        open(downloadpath, 'wb').write(r.content)\n",
    "        logging.info(\"file \"+downloadpath+\" downloaded\")\n",
    "    return downloadpath\n",
    "\n",
    "# TODO check for filetype and automatically extract zip file \n",
    "# so this can be re-used for the insta\n",
    " \n",
    "installFromUrl(\"https://github.com/christopheparisse/teicorpo/blob/689abf780eeb945a02f2c98c797af2417d562562/teicorpo.jar?raw=true\")\n",
    "installFromUrl(\"https://repo1.maven.org/maven2/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar\")\n",
    "pathToTeiCorpo=libDir+\"/*\"\n",
    "logging.info(pathToTeiCorpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68193a14",
   "metadata": {},
   "source": [
    "Collect all ELAN documents from pathToELANDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550786c9-6d7f-41ae-b4eb-0a9451ef87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELANDocs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da39ba7-a412-4777-81a6-671242bc18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDir(pathToDir):\n",
    "    docs = []\n",
    "    logging.info(\"processing \"+pathToDir)\n",
    "    for i in os.scandir(pathToDir):\n",
    "        filename=os.path.basename(i)\n",
    "        if i.is_dir():\n",
    "            dirname=os.path.basename(i)\n",
    "            dirDocs = processDir(pathToDir+\"/\"+dirname)\n",
    "            docs.extend(dirDocs)\n",
    "            \n",
    "        elif filename.endswith(\".eaf\"):\n",
    "            basename=Path(i).stem.replace(' ', '_') # this is a naive guard against spaces in filenames. They are unsupported.\n",
    "    \n",
    "            # check whether there is already a manually annotated TEI version of this ELAN document\n",
    "            TEI_annotated_filename=pathToAnnotatedTEIDir+\"/\"+basename+\".xml\"\n",
    "            TEI_annotated_exists = os.path.exists(os.path.abspath(TEI_annotated_filename)) \n",
    "            TEI_annotated=os.path.abspath(TEI_annotated_filename) if TEI_annotated_exists else False\n",
    "            \n",
    "            docs.append({\n",
    "                \"filepath\" : os.path.abspath(i), # path to the ELAN document\n",
    "                \"filename\" : filename,\n",
    "                \"basename\" : basename,\n",
    "                \"TEI_annotated\" : TEI_annotated,\n",
    "                \"tmpDir\" : False,  # path to temporary output files (e.g. output of TEICorpo)\n",
    "                \"filepath_tmp_TEI\" : False, # path to the output of TEICorpo\n",
    "                \"TEI\" : False # path to the TEI representation of the ELAN document with metadata from the spreadsheet\n",
    "                \n",
    "            })\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffbc7ac-ca02-4717-8306-b7ce59f2b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELANDocs=processDir(pathToELANDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f5f38-1511-4114-8212-198c91bd173b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in ELANDocs:\n",
    "    logging.info(d[\"filepath\"]+' -> '+d[\"basename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec70e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTEICorpo(docs = dict):\n",
    "    runtime = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    tmpDir = pathToTEIDir+\"/\"+runtime\n",
    "    os.makedirs(tmpDir, exist_ok=True)\n",
    "    for i in docs:\n",
    "        pathToInput = i[\"filepath\"]\n",
    "        filenameELAN = i[\"filename\"]\n",
    "        filenameTEI = i[\"basename\"]+\".xml\"\n",
    "        pathToOutput = tmpDir+\"/\"+\"ELAN_\"+filenameTEI\n",
    "        i[\"filepath_tmp_TEI\"] = os.path.abspath(pathToOutput)\n",
    "        i[\"tmpDir\"] = tmpDir\n",
    "        output = os.path.abspath(pathToTEIDir + \"/\" + i[\"basename\"] + \".xml\")\n",
    "        i[\"TEI\"] = os.path.abspath(output)\n",
    "        res = subprocess.run([\"java\", \"-cp\", pathToTeiCorpo, \"-Dfile.encoding=UTF-8\", \"fr.ortolang.teicorpo.TeiCorpo\", \"-from\",\"elan\", \"-to\",\"tei\", \"-o\",pathToOutput, pathToInput], capture_output=True, encoding=\"UTF-8\")\n",
    "        print(res.stdout)\n",
    "        print(res.stderr)\n",
    "        print(pathToOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38413b21",
   "metadata": {},
   "source": [
    "run TEI Corpo on all ELANDocs, writing the path to the TEI output back to the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2862351",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"runTEICorpo\" in SKIP_PROCESSING:\n",
    "    runTEICorpo(docs=ELANDocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51dc1f",
   "metadata": {},
   "source": [
    "## Step 5: Merge metadata and TEICorpo Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a604e-d031-474c-9f8c-9d27bd4cb8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mergeMetadata(docInfo, p):\n",
    "    \"\"\"Tries to find the corpus metadata in WIBARABCorpus.xml for the TEICorpo output by comparing its filename to the tei:title elements in it (= IDs of the Recording table), \n",
    "       and then replaces the teiHeader in the TEICorpo output with it.\"\"\"\n",
    "    # TOOD The matching logic should be revised, it's too messy \n",
    "    # probably move to the jupyter-notebook instead of having it here.\n",
    "    \n",
    "    # <xsl:variable name=\"corpusDoc\" select=\"doc($pathToCorpusDoc)\" as=\"document-node()\"/>\n",
    "    # <xsl:variable name=\"IDcandidates\" select=\"$corpusDoc//*:title\"/>\n",
    "    # <xsl:variable name=\"pathSegs\" select=\"tokenize(base-uri($input),'/')\"/>\n",
    "    # <xsl:variable name=\"recordingID\" select=\"$IDcandidates[some $x in $pathSegs satisfies contains(lower-case($x), lower-case(.))]\"/>\n",
    "    \n",
    "    # pathToTmpTEI: \n",
    "    pathToTmpTEI=docInfo[\"filepath_tmp_TEI\"]\n",
    "    pathToMergedTEI=docInfo[\"tmpDir\"]+\"/\"+docInfo[\"basename\"]+\"_00_metaMerged.xml\"\n",
    "    \n",
    "    logging.info(\"trying to inject metadata from \"+p[\"pathToCorpusDoc\"]+\" into \"+pathToTmpTEI)\n",
    "    \n",
    "    try:\n",
    "        transform(s=pathToTmpTEI, xsl=pathToStylesheetsDir+\"/mergeHeaderAndTranscription.xsl\", o=pathToMergedTEI, parameters=p)\n",
    "    except saxonche.PySaxonApiError as e:\n",
    "        logging.error(\"an error occured: \" + str(e) + \"\\n\" + s + \": \" + pathToPostprocessXSL + \" -> \" + s)\n",
    "    \n",
    "    # check wether the output file is well-formed\n",
    "    #try:\n",
    "    #    parsed = etree.parse(pathToMergedTEI)\n",
    "   #     if parsed:\n",
    "    docInfo[\"filepath_tmp_00_mergedMetadata\"]=pathToMergedTEI\n",
    "    return pathToMergedTEI\n",
    "    \n",
    "    #except etree.XMLSyntaxError as e:\n",
    "     #   logging.error(\"merge metadata resulted in an non-wellformed (empty?) XML document\")\n",
    "      #  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf59e5-5e62-4bd0-ae65-a303184540ee",
   "metadata": {},
   "source": [
    "## Step 6: Post-process merged TEI document prior to tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633ee75-f7c7-40be-8564-c475eaa0e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postProcessMergedTEI(docObject, pathToInput):\n",
    "    \"\"\"applies a post-process XSLT to the merged document prior to \"\"\"\n",
    "    s = pathToInput #docInfo[\"filepath_tmp_TEImergedMetadata\"]\n",
    "    o = s\n",
    "    logging.info(\"running post-metadata-merge processing on \"+s)\n",
    "    if not os.path.exists(os.path.abspath(s)):\n",
    "        logging.error(\"file \"+s+\" does not exist.\")\n",
    "    else:\n",
    "        pathToPostprocessXSL=pathToStylesheetsDir+\"/postprocessTEICorpoOutput.xsl\"\n",
    "        try:\n",
    "            transform(s, pathToPostprocessXSL, o, {})\n",
    "        except saxonche.PySaxonApiError as e:\n",
    "            logging.error(\"an error occured: \" + str(e) + \"\\n\" + s + \": \" + pathToPostprocessXSL + \" -> \" + o)\n",
    "    \n",
    "        # check wether the output file is well-formed\n",
    "        #try:\n",
    "        #    parsed = etree.parse(o)\n",
    "        #    if parsed:\n",
    "        docObject[\"filepath_tmp_TEImergedMetadata\"]=o\n",
    "        return o\n",
    "        #except etree.XMLSyntaxError as e:\n",
    "        #    logging.error(\"post-processing merged TEI document resulted in an non-wellformed (empty?) XML document\")\n",
    "        #    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a7c29-d7bb-47c7-ba8b-d1073168d67d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 7: Tokenization of unannotated texts\n",
    "\n",
    "Run a local copy of [xsl-tokenizer](https://github.com/acdh-oeaw/xsl-tokenizer)\n",
    "\n",
    "The merged TEI document is tokenized for further manual annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302385d4-f8b0-4f9c-bdfa-75a645e884f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 7.0: (Re-)generate tokenizer stylesheets (optional)\n",
    "\n",
    "Regenerate the XSLs used in the following steps.\n",
    "This can not be done with saxonpy (xincludes are not resolved)\n",
    "use\n",
    "```bash\n",
    "java -jar Saxon-HE-9.9.1-8.jar -s:profile.xml -xi:on -xsl:xsl/make_xsl.xsl\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfe02e61-57ec-4e03-b0d3-16cf145c27e6",
   "metadata": {},
   "source": [
    "def generateTokXSL(pathToProfile):\n",
    "    transform(s=pathToProfile, xsl=\"tokenizer/xsl/make_xsl.xsl\", o=\"./output.xsl\", parameters={\"output-base-path\":\"tokenizer\"})\n",
    "generateTokXSL(\"tokenizer/profile.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b9698-1946-4114-ba8b-5d2caca7b7f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "For all the ELAN files converted to TEI:\n",
    "\n",
    "### Step 7.1: Remove new lines\n",
    "\n",
    "Remove new lines and store to intermediate document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb48707-d990-4737-8115-277497add47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNL(docObject, pathToInput):\n",
    "    s = pathToInput # docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_00_metaMerged.xml\"\n",
    "    o = docObject[\"tmpDir\"]+'/'+docObject[\"basename\"]+\"_01_nlRmd.xml\"\n",
    "    logging.info(\"removing new lines from \"+s)\n",
    "    transform(s = s, xsl = \"./tokenizer/xsl/rmNl.xsl\", o = o)\n",
    "    # check wether the output file is well-formed\n",
    "    #try:\n",
    "    #    parsed = etree.parse(o)\n",
    "    #    if parsed:\n",
    "    docObject[\"filepath_tmp_t0_rmnl\"]=o\n",
    "    return o\n",
    "    #except etree.XMLSyntaxError as e:\n",
    "    #    logging.error(\"tokenizing step 0 / removing newlines resulted in an non-wellformed (empty?) XML document\")\n",
    "    #    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95efdd-eaaa-4132-a42b-e70f6a0225b5",
   "metadata": {},
   "source": [
    "### Step 7.2: create w tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a825ea8-f03d-4f56-b4ef-158817e5cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(docInfo, pathToInput):\n",
    "    s = pathToInput # docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_01_nlRmd.xml\"\n",
    "    logging.info(\"tokenizing \"+s)\n",
    "    o = docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_02_toks.xml\"\n",
    "    transform(s = s, xsl = \"./tokenizer/wrapper_toks.xsl\", o = o)\n",
    "    # check wether the output file is well-formed\n",
    "    #try:\n",
    "    #    parsed = etree.parse(o)\n",
    "    #    if parsed:\n",
    "    docInfo[\"filepath_tmp_t1_w\"]=o\n",
    "    return o\n",
    "    #except etree.XMLSyntaxError as e:\n",
    "    #    logging.error(\"tokenizing step 1 / tokenization resulted in an non-wellformed (empty?) XML document\")\n",
    "    #    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33998805-c4f2-4039-999b-419acb3e69f9",
   "metadata": {},
   "source": [
    "### Step 7.3: Add part attributes to w tags\n",
    "\n",
    "Add Part-Attributes and explicit token links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3946396-3a73-4180-873f-2361a669cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addP(docInfo, pathToInput):\n",
    "    s = pathToInput #docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_02_toks.xml\"\n",
    "    logging.info(\"adding @part on <w>\")\n",
    "    o = docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_03_tokenized.xml\"\n",
    "    transform(s = s, xsl = \"./tokenizer/wrapper_addP.xsl\", o = o)\n",
    "    # check wether the output file is well-formed\n",
    "    #try:\n",
    "    #    parsed = etree.parse(o)\n",
    "    #    if parsed:\n",
    "    docInfo[\"filepath_tmp_t2_part\"]=o\n",
    "    return o\n",
    "    #except etree.XMLSyntaxError as e:\n",
    "    #    logging.error(\"tokenizing step 2 / adding w/@part resulted in an non-wellformed (empty?) XML document\")\n",
    "    #    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729253b-f6e2-42b8-b4ef-e67319aed15b",
   "metadata": {},
   "source": [
    "### Step 7.4: apply project-specific post-processing\n",
    "\n",
    "Do some post tokenization processing specific to the Shawi project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6292bd18-6718-4d64-97c7-c4c093283f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postProcess(docInfo, pathToInput):\n",
    "    s = pathToInput #docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_03_tokenized.xml\"\n",
    "    logging.info(\"applying post-tokenization processing to \"+s)\n",
    "    o = docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_04_posttok.xml\"\n",
    "    transform(s = s, xsl = \"./tokenizer/postTokenization/1.xsl\", o = o)\n",
    "    # check wether the output file is well-formed\n",
    "    #try:\n",
    "    #    parsed = etree.parse(o)\n",
    "    #    if parsed:\n",
    "    docInfo[\"filepath_tmp_t3_post\"]=o\n",
    "    return o\n",
    "    #except etree.XMLSyntaxError as e:\n",
    "    #    logging.error(\"tokenizing step 4 / postprocessing resulted in an non-wellformed (empty?) XML document\")\n",
    "    #    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e721c-bb9e-4a7b-b6fd-be53e06b14b3",
   "metadata": {},
   "source": [
    "## Step 6.5: move token namespace from xtoks to TEI \n",
    "\n",
    "**--> This step creates the files which data curators will copy to `010_manannot` and annotate using the TEI enricher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2ec8a-fe7e-4142-9382-b5cd3bfd30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTEIForAnnotation(docInfo, pathToInput):\n",
    "    s = pathToInput # output of postProcess = docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_04_posttok.xml\"\n",
    "    o = docInfo[\"TEI\"]\n",
    "    logging.info(\"creating TEI document for annotation from \"+s)\n",
    "    transform(s = s, xsl = \"./tokenizer/custom_xtoks2tei.xsl\", o = o, parameters = {\"preserve-ws\": \"false\"})\n",
    "    \n",
    "    #try:\n",
    "    #    parsed = etree.parse(o)\n",
    "    #    if parsed:\n",
    "    return o\n",
    "    #except etree.XMLSyntaxError as e:\n",
    "    #    logging.error(\"tokenizing step 5 / custom_xtoks2tei resulted in an non-wellformed (empty?) XML document\")\n",
    "    #    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44acba-435d-449c-afc1-1288233036e1",
   "metadata": {},
   "source": [
    "## Step 7: Create NoSke input\n",
    "\n",
    "We create verticals from the unannotated texts and attach the token annotations from `010_manannot` to them.\n",
    "\n",
    "### Step 7.1 Create XML vertical from tokenized XML documents\n",
    "\n",
    "We take the tokenized XML document (prior to have moved to TEI) and create an XML vertical from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e07285-908a-4f6e-89ff-43b4f59e103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createXMLVert(docInfo, pathToInput):\n",
    "    s = pathToInput # output of postProcess = docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_04_posttok.xml\"\n",
    "    logging.info(\"creating XML vertical from \"+s)\n",
    "    o = docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_05_vert.xml\"\n",
    "    transform(s = s, xsl = \"./tokenizer/custom_xtoks2vert.xsl\", o = o)\n",
    "    \n",
    "    #try:\n",
    "    #    parsed = etree.parse(o)\n",
    "    #    if parsed:\n",
    "    return o\n",
    "    #except etree.XMLSyntaxError as e:\n",
    "    #    logging.error(\"tokenizing step 5 / custom_xtoks2tei resulted in an non-wellformed (empty?) XML document\")\n",
    "    #return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29a7e0-e42c-4e87-a443-17501b1191cc",
   "metadata": {},
   "source": [
    "### Step 7.2: attach manual annotations to the XML vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0cfa1-cba3-4480-b040-94749902527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attachAnnotationsToXMLVert(docInfo, pathToXMLVertical):\n",
    "    \"\"\"Try to add existing annotations to the newly converted document, if they exist.\"\"\"\n",
    "    if docInfo[\"TEI_annotated\"] and os.path.exists(os.path.abspath(docInfo[\"TEI_annotated\"])):\n",
    "        transform(\n",
    "            s = pathToXMLVertical, #docInfo[\"tmpDir\"]+'/'+docInfo[\"basename\"]+\"_05_vert.xml\",\n",
    "            xsl = pathToStylesheetsDir+\"/copyAnaToVert.xsl\", \n",
    "            o = docInfo[\"tmpDir\"] + \"/\" + docInfo[\"basename\"] + \"_05_vert_annot.xml\",\n",
    "            parameters = {\n",
    "                \"path_to_annotated_doc\": pathlib.Path(os.path.abspath(docInfo[\"TEI_annotated\"])).as_uri()\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\"No previous annotations found for \"+docInfo[\"basename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107ed8f-dfa4-41b8-9f24-c63a9c71589c",
   "metadata": {},
   "source": [
    "### Step 7.3 convert XML vertical to text vertical\n",
    "\n",
    "Create a vertical vor NoSkE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed763150-c07c-45d0-b077-05f8839e3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNoSkEVert(docInfo, pathToInput):\n",
    "    s = pathToInput # docInfo[\"tmpDir\"] + \"/\" + docInfo[\"basename\"] + \"_05_vert_annot.xml\"\n",
    "    o = noSkEVertDir + \"/\" + docInfo[\"basename\"] + \".txt\"\n",
    "    transform( s = s,xsl = \"./tokenizer/wrapper_vert2txt.xsl\", o = o)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b488db-49d5-4f4b-a81c-f4e2dbf712d0",
   "metadata": {},
   "source": [
    "## Run Steps 6- 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24c432-2ac1-48d8-a50f-604b34b808d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mergeParam = { \"pathToCorpusDoc\": pathlib.Path(os.path.abspath(pathToTeiCorpus)).as_uri() }\n",
    "for doc in ELANDocs:\n",
    "    logging.info(\"\\n\\n*** processing \"+doc[\"basename\"]+': '+doc[\"filepath_tmp_TEI\"]+\" -> \"+doc[\"TEI\"])\n",
    "    \n",
    "    mdMerged = mergeMetadata(doc, mergeParam)\n",
    "    if not mdMerged: \n",
    "        logging.error(\"mergeMetadata did not return expected value. Expected path to merged tmp TEI. returned value: \"+str(mdMerged))\n",
    "    else:\n",
    "        \n",
    "        mdMergedPostProcessed = postProcessMergedTEI(doc, mdMerged)\n",
    "        \n",
    "        if not mdMergedPostProcessed:\n",
    "            logging.error(\"mdMergedPostProcessed did not return expected value. Expected path, got \"+str(mdMergedPostProcessed))\n",
    "        \n",
    "        else:\n",
    "            nlRmved = removeNL(doc, mdMergedPostProcessed)\n",
    "            \n",
    "            if not nlRmved:\n",
    "                logging.error(\"removeNL did not return expected value. Expected path, got \"+str(mdMergedPostProcessed))\n",
    "            else:\n",
    "                \n",
    "                tokenized = tokenize(doc, nlRmved)\n",
    "\n",
    "                if not tokenized:\n",
    "                    logging.error(\"tokenize did not return expected value. Expected path, got \"+str(tokenized))\n",
    "                else:\n",
    "                    pAdded = addP(doc, tokenized)\n",
    "                    if not pAdded:\n",
    "                        logging.error(\"addP did not return expected value. Expected path, got \"+str(pAdded))\n",
    "\n",
    "                    else:\n",
    "                        tokenizedPostProcessed = postProcess(doc, pAdded)\n",
    "                        if not tokenizedPostProcessed:\n",
    "                            logging.error(\"postProcess did not return expected value. Expected path, got \"+str(tokenizedPostProcessed))\n",
    "                        else:\n",
    "                            \n",
    "                            teiForAnnotation = createTEIForAnnotation(doc, tokenizedPostProcessed)\n",
    "                            \n",
    "                            if not teiForAnnotation:\n",
    "                                logging.error(\"createTEIForAnnotation did not return expected value. Expected path, got \"+str(teiForAnnotation))\n",
    "                            \n",
    "                            xmlVert = createXMLVert(doc, tokenizedPostProcessed)\n",
    "                            if not xmlVert:\n",
    "                                logging.error(\"createXMLVert did not return expected value. Expected path, got \"+str(tokenizedPostProcessed))\n",
    "                            \n",
    "                            annotationsAttached = attachAnnotationsToXMLVert(doc, xmlVert)\n",
    "                            if annotationsAttached:\n",
    "                                createNoSkEVert(doc, annotationsAttached)\n",
    "                            else:\n",
    "                                createNoSkEVert(doc, xmlVert)\n",
    "                            \n",
    "                            logging.info(doc[\"basename\"]+\": done.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb522d",
   "metadata": {},
   "source": [
    "## Replace TEI elements with x-includes in corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1045b01-72db-4104-8ebe-e4c4dffa385d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b0b902fac6ad7dd00456e1f7dc72379c0baf1ab5135d56a56b79f9771306c5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
